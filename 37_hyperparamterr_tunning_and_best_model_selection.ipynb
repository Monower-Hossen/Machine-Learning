{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learing 37: Selecting the Best Machine Learning Model with the Best Hyperparameters\n",
    "\n",
    "\n",
    "## 1. **Model Selection Basics**\n",
    "\n",
    "### Why choosing the right model is important\n",
    "\n",
    "Different ML models have different strengths.\n",
    "\n",
    "* **Linear models (Logistic Regression, Linear Regression):** Simple, interpretable, fast, but may underfit complex problems.\n",
    "* **Tree-based models (Random Forest, XGBoost):** Handle nonlinear relationships, robust, but may overfit if not tuned.\n",
    "* **Neural networks:** Very powerful, but need large datasets and high compute.\n",
    "\n",
    "Choosing the wrong model may lead to **underfitting** (too simple) or **overfitting** (too complex).\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "\n",
    "* **Bias**: Error from oversimplification (e.g., using a linear model for non-linear data).\n",
    "* **Variance**: Error from sensitivity to noise (e.g., an overfitted deep tree).\n",
    "* The **goal** is to balance them: low enough bias + low enough variance.\n",
    "\n",
    "\n",
    "## 2. **Hyperparameters vs Parameters**\n",
    "\n",
    "* **Parameters:** Learned from data during training.\n",
    "\n",
    "  * Example: In Linear Regression, the coefficients (weights).\n",
    "  * Example: In Neural Networks, the weights of connections.\n",
    "\n",
    "* **Hyperparameters:** Set *before* training, not learned.\n",
    "\n",
    "  * Example: Learning rate in gradient descent.\n",
    "  * Example: Number of trees in Random Forest.\n",
    "  * Example: Maximum depth of XGBoost tree.\n",
    "\n",
    "- **Parameters** = learned values.\n",
    "- **Hyperparameters** = tuning knobs chosen by us.\n",
    "\n",
    "\n",
    "## 3. **Techniques for Model Selection**\n",
    "\n",
    "* **Train-Test Split:**\n",
    "  Split dataset (e.g., 80% train, 20% test). Train on train, evaluate on test.\n",
    "  - Simple, but may give unstable results for small datasets.\n",
    "\n",
    "* **Cross-Validation (CV):**\n",
    "\n",
    "  * Split into *k folds* (e.g., k=5).\n",
    "  * Train on 4 folds, validate on 1 fold. Repeat for all folds.\n",
    "  * Average the performance.\n",
    "    - More reliable than train-test split.\n",
    "\n",
    "* **Nested Cross-Validation:**\n",
    "\n",
    "  * Outer loop ‚Üí evaluate generalization.\n",
    "  * Inner loop ‚Üí tune hyperparameters.\n",
    "    - Prevents overfitting during hyperparameter search.\n",
    "\n",
    "## 4. **Hyperparameter Tuning Approaches**\n",
    "\n",
    "* **Grid Search:** Try all combinations (exhaustive).\n",
    "  ‚úÖ Finds best within given grid.\n",
    "  ‚ùå Expensive for large search spaces.\n",
    "\n",
    "* **Random Search:** Randomly sample combinations.\n",
    "  ‚úÖ Often better than grid for large spaces.\n",
    "  ‚ùå May miss best exact combination.\n",
    "\n",
    "* **Bayesian Optimization:** Uses past results to guide next search (smart).\n",
    "  ‚úÖ Efficient, fewer trials needed.\n",
    "  ‚ùå More complex.\n",
    "\n",
    "* **Genetic Algorithms (Evolutionary Search):** Mimics natural selection.\n",
    "  ‚úÖ Good for large non-smooth spaces.\n",
    "  ‚ùå Slower.\n",
    "\n",
    "* **Modern approaches (Hyperband, Optuna):**\n",
    "\n",
    "  * **Hyperband:** Early-stop bad configurations.\n",
    "  * **Optuna:** Combines Bayesian + pruning.\n",
    "    ‚úÖ Very efficient for large hyperparameter spaces.\n",
    "\n",
    "## 5. **Evaluation Metrics**\n",
    "\n",
    "Choosing the right metric depends on the problem:\n",
    "\n",
    "* **Classification:**\n",
    "\n",
    "  * Accuracy ‚Üí Good for balanced classes.\n",
    "  * Precision, Recall, F1 ‚Üí Better for imbalanced classes.\n",
    "  * ROC-AUC ‚Üí Measures ranking ability.\n",
    "\n",
    "* **Regression:**\n",
    "\n",
    "  * MAE (Mean Absolute Error) ‚Üí Average error.\n",
    "  * MSE (Mean Squared Error) ‚Üí Penalizes large errors.\n",
    "  * R¬≤ (coefficient of determination) ‚Üí Proportion of variance explained.\n",
    "\n",
    " Always match the metric with your business/real-world goal.\n",
    "\n",
    "\n",
    "## 6. **Model Comparison**\n",
    "\n",
    "* Use **cross-validation scores** to compare.\n",
    "* Apply **statistical tests** (e.g., paired t-test on CV folds) to ensure differences are significant.\n",
    "* Consider **interpretability, training time, inference speed**, not just accuracy.\n",
    "\n",
    "\n",
    "## 7. **Avoiding Pitfalls**\n",
    "\n",
    "- **Data Leakage:** Using future/test info in training (e.g., scaling test data with train mean).\n",
    "- **Overfitting during tuning:** If you tune on test set ‚Üí overly optimistic performance.\n",
    "- Always keep a separate **final test set**.\n",
    "- **Ignoring class imbalance:** High accuracy may be misleading.\n",
    "\n",
    "\n",
    "### What this code does:\n",
    "\n",
    "1. Loads **Breast Cancer dataset** (binary classification).\n",
    "2. Splits into train/test.\n",
    "3. Defines Logistic Regression, Random Forest, and XGBoost with small hyperparameter grids.\n",
    "4. Uses **GridSearchCV with cross-validation** to find best hyperparameters.\n",
    "5. Compares models on the test set.\n",
    "\n",
    "\n",
    "### This workflow ensures you:\n",
    "\n",
    "1. Choose models wisely.\n",
    "2. Tune hyperparameters effectively.\n",
    "3. Evaluate with the right metrics.\n",
    "4. Avoid common mistakes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rergression Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset\n",
    "diamonds = sns.load_dataset(\"diamonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocess dataset\n",
    "# Convert categorical variables to numeric (Label Encoding)\n",
    "label_cols = [\"cut\", \"color\", \"clarity\"]\n",
    "diamonds[label_cols] = diamonds[label_cols].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Target\n",
    "X = diamonds.drop(\"price\", axis=1)\n",
    "y = diamonds[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (not mandatory for tree-based models but helps linear models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Lasso Regression\": Lasso(alpha=0.001),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Train & Evaluate\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    if \"Linear\" in name or \"Ridge\" in name or \"Lasso\" in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    results.append([name, r2, rmse, mae])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Diamonds Dataset:\n",
      "\n",
      "               Model  R2 Score         RMSE         MAE\n",
      "4      Random Forest  0.981517   542.047468  267.975502\n",
      "6            XGBoost  0.981276   545.574239  277.941284\n",
      "5  Gradient Boosting  0.972955   655.694773  364.865262\n",
      "3      Decision Tree  0.966566   729.033563  355.644234\n",
      "1   Ridge Regression  0.885140  1351.262011  858.800082\n",
      "0  Linear Regression  0.885140  1351.263480  858.708470\n",
      "2   Lasso Regression  0.885140  1351.263537  858.709208\n"
     ]
    }
   ],
   "source": [
    "# 5. Show Results\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"R2 Score\", \"RMSE\", \"MAE\"])\n",
    "print(\"\\nModel Performance on Diamonds Dataset:\\n\")\n",
    "print(results_df.sort_values(by=\"R2 Score\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Best Models by Metric\n",
    "best_r2 = results_df.loc[results_df[\"R2 Score\"].idxmax()]\n",
    "best_rmse = results_df.loc[results_df[\"RMSE\"].idxmin()]\n",
    "best_mae = results_df.loc[results_df[\"MAE\"].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Models by Metrics:\n",
      "üëâ Highest R2 Score: Random Forest (0.9815)\n",
      "üëâ Lowest RMSE: Random Forest (542.05)\n",
      "üëâ Lowest MAE: Random Forest (267.98)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest Models by Metrics:\")\n",
    "print(f\"üëâ Highest R2 Score: {best_r2['Model']} ({best_r2['R2 Score']:.4f})\")\n",
    "print(f\"üëâ Lowest RMSE: {best_rmse['Model']} ({best_rmse['RMSE']:.2f})\")\n",
    "print(f\"üëâ Lowest MAE: {best_mae['Model']} ({best_mae['MAE']:.2f})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "\n",
      "Classifier: Decision Tree\n",
      "Mean Accuracy: 0.9533333333333335\n",
      "\n",
      "Classifier: Random Forest\n",
      "Mean Accuracy: 0.9600000000000002\n",
      "\n",
      "Classifier: SVM\n",
      "Mean Accuracy: 0.9666666666666668\n",
      "\n",
      "Classifier: KNN\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# dont show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a dictionary of classifiers to evaluate\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Perform k-fold cross-validation and calculate the mean accuracy\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    scores = cross_val_score(classifier, X, y, cv=kfold)\n",
    "    accuracy = np.mean(scores)\n",
    "    print(\"Classifier:\", name)\n",
    "    print(\"Mean Accuracy:\", accuracy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression MSE:  1825912.9915253515\n",
      "LinearRegression R2:  0.8851397433679629\n",
      "LinearRegression MAE:  858.7084697710105\n",
      "\n",
      "\n",
      "LinearRegression Best Params:  {}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import SVR, KNeighborsRegressor, and XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a dictionaries of list of models to evaluate performance with hyperparameters\n",
    "models = { \n",
    "          'LinearRegression' : (LinearRegression(), {}),\n",
    "          'SVR' : (SVR(), {'kernel': ['rbf', 'poly', 'sigmoid']}),\n",
    "          'DecisionTreeRegressor' : (DecisionTreeRegressor(), {'max_depth': [None, 5, 10]}),\n",
    "          'RandomForestRegressor' : (RandomForestRegressor(), {'n_estimators': [10, 100]}),\n",
    "          'KNeighborsRegressor' : (KNeighborsRegressor(), {'n_neighbors': np.arange(3, 100, 2)}),\n",
    "          'GradientBoostingRegressor' : (GradientBoostingRegressor(), {'n_estimators': [10, 100]}),\n",
    "          'XGBRegressor' : (XGBRegressor(), {'n_estimators': [10, 100]}),          \n",
    "          }\n",
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    # create a pipline\n",
    "    pipeline = GridSearchCV(model, params, cv=5)\n",
    "    \n",
    "    # fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # print the performing metric\n",
    "    print(name, 'MSE: ', mean_squared_error(y_test, y_pred))\n",
    "    print(name, 'R2: ', r2_score(y_test, y_pred))\n",
    "    print(name, 'MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print(name, 'Best Params: ', pipeline.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionaries of list of models to evaluate performance with hyperparameters\n",
    "models = { \n",
    "          'LinearRegression' : (LinearRegression(), {}),\n",
    "          'SVR' : (SVR(), {'kernel': ['rbf', 'poly', 'sigmoid'], 'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01], 'epsilon': [0.1, 0.01, 0.001]}),\n",
    "          'DecisionTreeRegressor' : (DecisionTreeRegressor(), {'max_depth': [None, 5, 10], 'splitter': ['best', 'random']}),\n",
    "          'RandomForestRegressor' : (RandomForestRegressor(), {'n_estimators': [10, 100, 1000], 'max_depth': [None, 5, 10]}),\n",
    "          'KNeighborsRegressor' : (KNeighborsRegressor(), {'n_neighbors': np.arange(3, 100, 2), 'weights': ['uniform', 'distance']}),\n",
    "          'GradientBoostingRegressor' : (GradientBoostingRegressor(), {'loss': ['ls', 'lad', 'huber', 'quantile'], 'n_estimators': [10, 100, 1000]}),\n",
    "          'XGBRegressor' : (XGBRegressor(), {'n_estimators': [10, 100, 1000], 'learning_rate': [0.1, 0.01, 0.001]}),          \n",
    "          }\n",
    "\n",
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    # create a pipline\n",
    "    pipeline = GridSearchCV(model, params, cv=5)\n",
    "    \n",
    "    # fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "      \n",
    "    # print the performing metric\n",
    "    print(name, 'MSE: ', mean_squared_error(y_test, y_pred))\n",
    "    print(name, 'R2: ', r2_score(y_test, y_pred))\n",
    "    print(name, 'MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Add preprocessor inside the pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a preprocessor\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('numeric_scaling', StandardScaler(), ['total_bill', 'size'])], remainder='passthrough')\n",
    "\n",
    "\n",
    "# Create a dictionaries of list of models to evaluate performance with hyperparameters\n",
    "models = { \n",
    "          'LinearRegression' : (LinearRegression(), {}),\n",
    "          'SVR' : (SVR(), {'kernel': ['rbf', 'poly', 'sigmoid'], 'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01], 'epsilon': [0.1, 0.01, 0.001]}),\n",
    "          'DecisionTreeRegressor' : (DecisionTreeRegressor(), {'max_depth': [None, 5, 10], 'splitter': ['best', 'random']}),\n",
    "          'RandomForestRegressor' : (RandomForestRegressor(), {'n_estimators': [10, 100, 1000], 'max_depth': [None, 5, 10]}),\n",
    "          'KNeighborsRegressor' : (KNeighborsRegressor(), {'n_neighbors': np.arange(3, 100, 2), 'weights': ['uniform', 'distance']}),\n",
    "          'GradientBoostingRegressor' : (GradientBoostingRegressor(), {'loss': ['ls', 'lad', 'huber', 'quantile'], 'n_estimators': [10, 100, 1000]}),\n",
    "          'XGBRegressor' : (XGBRegressor(), {'n_estimators': [10, 100, 1000], 'learning_rate': [0.1, 0.01, 0.001]}),          \n",
    "          }\n",
    "\n",
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    # create a pipline with preprocessor\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])   \n",
    "    \n",
    "    # make a grid search cv to tune the hyperparameter\n",
    "    grid_search = GridSearchCV(pipeline, params, cv=5)\n",
    "    \n",
    "    \n",
    "    # fit the pipeline\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    \n",
    "      \n",
    "    # print the performing metric\n",
    "    print(name, 'MSE: ', mean_squared_error(y_test, y_pred))\n",
    "    print(name, 'R2: ', r2_score(y_test, y_pred))\n",
    "    print(name, 'MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
